{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration de Cuda dans Google Colab"
      ],
      "metadata": {
        "id": "qE-HbkLWqjJK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJjU5zI_-tjl"
      },
      "outputs": [],
      "source": [
        "!nvcc -V"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "id": "HBZjh4P4-1HE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9eee920-7de2-43be-ca10-f1b6d0fd1a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-vhp9n03l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-vhp9n03l\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4305 sha256=9307ebc265eb3c3c277a0a9c8632eb909bb66f032d97b953d5a3ca00e5dbf81e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pa486kt2/wheels/db/c1/1f/a2bb07bbb4a1ce3c43921252aeafaa6205f08637e292496f04\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On vérifie que l'on est bien connecté au GPU"
      ],
      "metadata": {
        "id": "YJw3IWdqtpfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "9gl_4Pn7_JR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du plugin nvcc permettant de compiler/executer les programmes Cuda"
      ],
      "metadata": {
        "id": "NlBBvVVOt4Bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "id": "6YGSePh_Q_DP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4381a430-9a51-41e4-80b8-93898fc68e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Un makefile est déjà à votre disposition pour compiler les programme du TP\n"
      ],
      "metadata": {
        "id": "0zrHyBuVuQpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Executez la cellule du Makefile\n",
        "\n",
        "Le makefile a été modifié pour les programmes puisse s'exécuter avec la GPU premium.\n",
        "\n"
      ],
      "metadata": {
        "id": "AWS3uSGwu2o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Makefile\n",
        "# Change the example variable to build a different source module (e.g. EXAMPLE=exercice01)\n",
        "EXAMPLE=program\n",
        "\n",
        "# Makefile variables \n",
        "# Add extra targets to OBJ with space separator e.g. If there is as source file random.c then add random.o to OBJ)\n",
        "# Add any additional dependancies (header files) to DEPS. e.g. if there is aheader file random.h required by your source modules then add this to DEPS.\n",
        "CC=gcc\n",
        "CFLAGS= -O3 -Wextra -fopenmp\n",
        "NVCC=nvcc\n",
        "NVCC_FLAGS= -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_87,code=sm_87\n",
        "OBJ=$(EXAMPLE).o\n",
        "DEPS=\n",
        "\n",
        "# Build rule for object files ($@ is left hand side of rule, $< is first item from the right hand side of rule)\n",
        "%.o : %.cu $(DEPS)\n",
        "\t$(NVCC) -c -o $@ $< $(NVCC_FLAGS) $(addprefix -Xcompiler ,$(CCFLAGS))\n",
        "\n",
        "# Make example ($^ is all items from right hand side of the rule)\n",
        "$(EXAMPLE) : $(OBJ)\n",
        "\t$(NVCC) -o $@ $^ $(NVCC_FLAGS) $(addprefix -Xcompiler ,$(CCFLAGS))\n",
        "\n",
        "# PHONY prevents make from doing something with a filename called clean\n",
        ".PHONY : clean\n",
        "clean:\n",
        "\trm -rf $(EXAMPLE) $(OBJ)"
      ],
      "metadata": {
        "id": "2VMs5wjdRU0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834045a9-174e-4caf-b544-7b11c21d9395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP4\n"
      ],
      "metadata": {
        "id": "ri3OU-i0vt2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.h\n",
        "#ifndef __UTILS_H__\n",
        "#define __UTILS_H__\n",
        "#include <stdio.h>\n",
        "\n",
        "static void HandleError( cudaError_t err,\n",
        "                         const char *file,\n",
        "                         int line ) {\n",
        "    if (err != cudaSuccess) {\n",
        "        printf( \"%s in %s at line %d\\n\", cudaGetErrorString( err ),\n",
        "                file, line );\n",
        "        exit( EXIT_FAILURE );\n",
        "    }\n",
        "}\n",
        "#define HANDLE_ERROR( err ) (HandleError( err, __FILE__, __LINE__ ))\n",
        "\n",
        "\n",
        "#define HANDLE_NULL( a ) {if (a == NULL) { \\\n",
        "                            printf( \"Host memory failed in %s at line %d\\n\", \\\n",
        "                                    __FILE__, __LINE__ ); \\\n",
        "                            exit( EXIT_FAILURE );}}\n",
        "\n",
        "template< typename T >\n",
        "void swap( T& a, T& b ) {\n",
        "    T t = a;\n",
        "    a = b;\n",
        "    b = t;\n",
        "}\n",
        "\n",
        "\n",
        "void* big_random_block( int size ) {\n",
        "    unsigned char *data = (unsigned char*)malloc( size );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "int* big_random_block_int( int size ) {\n",
        "    int *data = (int*)malloc( size * sizeof(int) );\n",
        "    HANDLE_NULL( data );\n",
        "    for (int i=0; i<size; i++)\n",
        "        data[i] = rand();\n",
        "\n",
        "    return data;\n",
        "}\n",
        "\n",
        "\n",
        "// a place for common kernels - starts here\n",
        "\n",
        "__device__ unsigned char value( float n1, float n2, int hue ) {\n",
        "    if (hue > 360)      hue -= 360;\n",
        "    else if (hue < 0)   hue += 360;\n",
        "\n",
        "    if (hue < 60)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*hue/60));\n",
        "    if (hue < 180)\n",
        "        return (unsigned char)(255 * n2);\n",
        "    if (hue < 240)\n",
        "        return (unsigned char)(255 * (n1 + (n2-n1)*(240-hue)/60));\n",
        "    return (unsigned char)(255 * n1);\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( unsigned char *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset*4 + 0] = value( m1, m2, h+120 );\n",
        "    optr[offset*4 + 1] = value( m1, m2, h );\n",
        "    optr[offset*4 + 2] = value( m1, m2, h -120 );\n",
        "    optr[offset*4 + 3] = 255;\n",
        "}\n",
        "\n",
        "__global__ void float_to_color( uchar4 *optr,\n",
        "                              const float *outSrc ) {\n",
        "    // map from threadIdx/BlockIdx to pixel position\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int y = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "    int offset = x + y * blockDim.x * gridDim.x;\n",
        "\n",
        "    float l = outSrc[offset];\n",
        "    float s = 1;\n",
        "    int h = (180 + (int)(360.0f * outSrc[offset])) % 360;\n",
        "    float m1, m2;\n",
        "\n",
        "    if (l <= 0.5f)\n",
        "        m2 = l * (1 + s);\n",
        "    else\n",
        "        m2 = l + s - l * s;\n",
        "    m1 = 2 * l - m2;\n",
        "\n",
        "    optr[offset].x = value( m1, m2, h+120 );\n",
        "    optr[offset].y = value( m1, m2, h );\n",
        "    optr[offset].z = value( m1, m2, h -120 );\n",
        "    optr[offset].w = 255;\n",
        "}\n",
        "\n",
        "\n",
        "#if _WIN32\n",
        "    //Windows threads.\n",
        "    #include <windows.h>\n",
        "\n",
        "    typedef HANDLE CUTThread;\n",
        "    typedef unsigned (WINAPI *CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC unsigned WINAPI\n",
        "    #define  CUT_THREADEND return 0\n",
        "\n",
        "#else\n",
        "    //POSIX threads.\n",
        "    #include <pthread.h>\n",
        "\n",
        "    typedef pthread_t CUTThread;\n",
        "    typedef void *(*CUT_THREADROUTINE)(void *);\n",
        "\n",
        "    #define CUT_THREADPROC void\n",
        "    #define  CUT_THREADEND\n",
        "#endif\n",
        "\n",
        "//Create thread.\n",
        "CUTThread start_thread( CUT_THREADROUTINE, void *data );\n",
        "\n",
        "//Wait for thread to finish.\n",
        "void end_thread( CUTThread thread );\n",
        "\n",
        "//Destroy thread.\n",
        "void destroy_thread( CUTThread thread );\n",
        "\n",
        "//Wait for multiple threads.\n",
        "void wait_for_threads( const CUTThread *threads, int num );\n",
        "\n",
        "#if _WIN32\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void *data){\n",
        "        return CreateThread(NULL, 0, (LPTHREAD_START_ROUTINE)func, data, 0, NULL);\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        WaitForSingleObject(thread, INFINITE);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        TerminateThread(thread, 0);\n",
        "        CloseHandle(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        WaitForMultipleObjects(num, threads, true, INFINITE);\n",
        "\n",
        "        for(int i = 0; i < num; i++)\n",
        "            CloseHandle(threads[i]);\n",
        "    }\n",
        "\n",
        "#else\n",
        "    //Create thread\n",
        "    CUTThread start_thread(CUT_THREADROUTINE func, void * data){\n",
        "        pthread_t thread;\n",
        "        pthread_create(&thread, NULL, func, data);\n",
        "        return thread;\n",
        "    }\n",
        "\n",
        "    //Wait for thread to finish\n",
        "    void end_thread(CUTThread thread){\n",
        "        pthread_join(thread, NULL);\n",
        "    }\n",
        "\n",
        "    //Destroy thread\n",
        "    void destroy_thread( CUTThread thread ){\n",
        "        pthread_cancel(thread);\n",
        "    }\n",
        "\n",
        "    //Wait for multiple threads\n",
        "    void wait_for_threads(const CUTThread * threads, int num){\n",
        "        for(int i = 0; i < num; i++)\n",
        "            end_thread( threads[i] );\n",
        "    }\n",
        "\n",
        "#endif\n",
        "\n",
        "#endif  // __UTILS_H__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8DiZxqTVzyD",
        "outputId": "c839b998-d917-4d27-e00c-52a431626bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Mémoire paginée vs. Mémoire épinglée.\n",
        "\n",
        "Afin de visualiser le gain que l'on obtient en utilisant la mémoire épinglée, vous allez implémentez deux fonctions **cuda_malloc(int n, bool up)** et **cuda_host_alloc(int n, bool up)**\n",
        "\n",
        "\n",
        "\n",
        "### cuda_malloc() \n",
        "#### 1.1 Allouez la mémoire hôte pour le vecteur a à l'aide de malloc\n",
        "#### 1.2 Allouez la mémoire GPU pour le vecteur dev_A à l'aide de cudaMalloc\n",
        "#### 1.3 Ecrire une boucle pour i allant de 0 à 100 :\n",
        "  - si up == True alors copiez le vecteur a dans dev_a\n",
        "  - sinon copiez le vecteur dev_a dans a.\n",
        "\n",
        "#### 1.4 Libérez la mémoire.\n",
        "\n",
        "### cuda_host_alloc()\n",
        "#### 1.4 Allouez la mémoire hôte pour le vecteur a à l'aide de malloc\n",
        "#### 1.5 Allouez la mémoire GPU pour le vecteur dev_A à l'aide de cudaMalloc\n",
        "#### 1.6 Ecrire une boucle pour i allant de 0 à 100 :\n",
        "  - si up == True alors copiez le vecteur a dans dev_a\n",
        "  - sinon copiez le vecteur dev_a dans a.\n",
        "\n",
        "#### 1.7 Libérez la mémoire.\n",
        "\n",
        "Constatez-vous une différence entre la mémoire paginée et la mémoire épinglée ?\n"
      ],
      "metadata": {
        "id": "uB1BFcg5Qxb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile program.cu\n",
        "#include \"utils.h\"\n",
        "\n",
        "#define N  (64*1024*1024)\n",
        "\n",
        "\n",
        "float cuda_malloc( int n, bool up ) {\n",
        "    cudaEvent_t     start, stop;\n",
        "    int             *a, *dev_a;\n",
        "    float           elapsedTime;\n",
        "\n",
        "    HANDLE_ERROR( cudaEventCreate( &start ) );\n",
        "    HANDLE_ERROR( cudaEventCreate( &stop ) );\n",
        "\n",
        "    // 1.1 Allouez de la mémoire hôte pour le vecteur a de taille n (malloc)\n",
        "    HANDLE_NULL( a );\n",
        "    // 1.2 Allouez de la mémoire GPU pour dev_a de taille n\n",
        "\n",
        "    HANDLE_ERROR( cudaEventRecord( start, 0 ) );\n",
        "    // 1.3 Pour i allant de 0 à 100 : si up == True alors copiez le vecteur a dans dev_a, sinon copiez le vecteur dev_a dans a.\n",
        "    \n",
        "    \n",
        "    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );\n",
        "    HANDLE_ERROR( cudaEventSynchronize( stop ) );\n",
        "    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,\n",
        "                                        start, stop ) );\n",
        "\n",
        "    // 1.4 Libérez la mémoire.\n",
        "    HANDLE_ERROR( cudaEventDestroy( start ) );\n",
        "    HANDLE_ERROR( cudaEventDestroy( stop ) );\n",
        "\n",
        "    return elapsedTime;\n",
        "}\n",
        "\n",
        "\n",
        "float cuda_host_alloc( int n, bool up ) {\n",
        "    cudaEvent_t     start, stop;\n",
        "    int             *a, *dev_a;\n",
        "    float           elapsedTime;\n",
        "\n",
        "    HANDLE_ERROR( cudaEventCreate( &start ) );\n",
        "    HANDLE_ERROR( cudaEventCreate( &stop ) );\n",
        "\n",
        "    // 1.5 Allouez de la mémoire hôte dans la mémoire épinglée pour le vecteur a de taille n (cudaHostAlloc)\n",
        "    // 1.6 Allouez de la mémoire GPU pour dev_a de taille n\n",
        "\n",
        "    HANDLE_ERROR( cudaEventRecord( start, 0 ) );\n",
        "    \n",
        "    // 1.7 Pour i allant de 0 à 100 : si up == True alors copiez le vecteur a dans dev_a, sinon copiez le vecteur dev_a dans a.\n",
        "    HANDLE_ERROR( cudaEventRecord( stop, 0 ) );\n",
        "    HANDLE_ERROR( cudaEventSynchronize( stop ) );\n",
        "    HANDLE_ERROR( cudaEventElapsedTime( &elapsedTime,\n",
        "                                        start, stop ) );\n",
        "\n",
        "    // 1.8 Libérez la mémoire.\n",
        "    HANDLE_ERROR( cudaEventDestroy( start ) );\n",
        "    HANDLE_ERROR( cudaEventDestroy( stop ) );\n",
        "\n",
        "    return elapsedTime;\n",
        "}\n",
        "\n",
        "\n",
        "int main( void ) {\n",
        "    float           elapsedTime;\n",
        "    float           MB = (float)100*N*sizeof(int)/1024/1024;\n",
        "\n",
        "\n",
        "    // try it with cudaMalloc\n",
        "    elapsedTime = cuda_malloc_test( N, true );\n",
        "    printf( \"Time using cudaMalloc:  %3.1f ms\\n\",\n",
        "            elapsedTime );\n",
        "    printf( \"\\tMB/s during copy up:  %3.1f\\n\",\n",
        "            MB/(elapsedTime/1000) );\n",
        "\n",
        "    elapsedTime = cuda_malloc_test( N, false );\n",
        "    printf( \"Time using cudaMalloc:  %3.1f ms\\n\",\n",
        "            elapsedTime );\n",
        "    printf( \"\\tMB/s during copy down:  %3.1f\\n\",\n",
        "            MB/(elapsedTime/1000) );\n",
        "\n",
        "    // now try it with cudaHostAlloc\n",
        "    elapsedTime = cuda_host_alloc_test( N, true );\n",
        "    printf( \"Time using cudaHostAlloc:  %3.1f ms\\n\",\n",
        "            elapsedTime );\n",
        "    printf( \"\\tMB/s during copy up:  %3.1f\\n\",\n",
        "            MB/(elapsedTime/1000) );\n",
        "\n",
        "    elapsedTime = cuda_host_alloc_test( N, false );\n",
        "    printf( \"Time using cudaHostAlloc:  %3.1f ms\\n\",\n",
        "            elapsedTime );\n",
        "    printf( \"\\tMB/s during copy down:  %3.1f\\n\",\n",
        "            MB/(elapsedTime/1000) );\n",
        "}"
      ],
      "metadata": {
        "id": "Rm-7gDO8BGH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4042a060-ad92-4164-e780-c50012b142a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing program.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "id": "PfakPQ1CHLUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./program"
      ],
      "metadata": {
        "id": "F4BRhsCtHLo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Streams et execution asynchrone\n",
        "\n",
        "Afin de visualiser le gain que l'on obtient en utilisant la mémoire épinglée, vous allez implémentez deux fonctions **cuda_malloc(int n, bool up)** et **cuda_host_alloc(int n, bool up)**\n",
        "\n",
        "\n",
        "\n",
        "### Mémoire paginée vs. épinglée\n",
        "  Pour le moment la mémoire pour les vecteurs host h_x, h_y et h_y1 est alloué via la fonction malloc. La mémoire alloué par malloc est paginable.\n",
        "  \n",
        "#### 2.1 Compilez , executez le programme et relevez le temps d'execution en sortie du programme.\n",
        "#### 2.2 Modifiez le code de sorte à n'utiliser que de la mémoire épingler via cudaHostMalloc(). \n",
        "N'oubliez pas de changer la façon dont on libère la mémoire en fin de programme.\n",
        "\n",
        "Executez à nouveau le programme, que constatez-vous sur le temps d'exécution ?\n",
        "\n",
        "### Streams\n",
        "Vous allez maintenant modifier le programme afin de rendre possible l'execution asynchrone entre les copies CPU <--> GPU et les kernels.\n",
        "\n",
        "Le but etant de diviser le vecteurs 1D h_x en un sous_ensemble de vecteurs definit par la variable constante subpart. Un stream s'occupera d'un sous-vecteur à la fois.\n",
        "\n",
        "```c\n",
        "typedef float ft;\n",
        "const int sub_parts = 64;\n",
        "const size_t ds = 1024*1024*sub_parts;\n",
        "const int count = 22;\n",
        "const int num_streams = 8;\n",
        "```\n",
        "\n",
        "Dans le code, differentes constantes ont été déclarés:\n",
        "\n",
        "- **ft** : type utilisé pour declarer nos variables dans le code (changer float par double pour utiliser la précision double)\n",
        "- **sub_paths** : Permet de diviser notre vecteur en un sous-ensemble de vecteurs.\n",
        "- **ds** : Correspond à la taille totale du vecteur. Dans la partie streams du TP, sub_parts permet d'avoir 64 vecteurs de taille 1024*1024 \n",
        "- **count** : Permet de définir la taille de l'interval de valeurs utilisé pour calculer la moyenne de la probabilité de densité.\n",
        "- **num_streams** : Correspond au nombre de streams que l'on veut lancer.\n",
        "\n",
        "Vous ecrirez le code correspondant aux streams dans la partie délimité par **#ifdef USE_STREAM #endif**\n",
        "\n",
        "```c\n",
        "#ifdef USE_STREAMS\n",
        "\n",
        "\t// Code correspondant aux streams\n",
        "\n",
        "#endif\n",
        "```\n",
        "\n",
        "#### 2.3 Creation des streams\n",
        "\n",
        "Ecrivez le ou les instructions code permettant de creer vos streams de tailles num_streams en utilisant la fonction **cudaStreamCreate()**\n",
        "\n",
        "#### 2.4 Destruction des streams\n",
        "\n",
        "Ecrivez le ou les instructions permettant de détruire vos streams en utilisant la fonction **cudaStreamDestroy()**.\n",
        "\n",
        "#### 2.5 Execution des streams\n",
        "\n",
        "Pour chaque stream : \n",
        "\n",
        "- Faite une copie asynchrone CPU vers GPU du vecteur **h_x** dans **d_x** via **cudaMemcpyAsync()**.\n",
        "- Lancer le kernel pour le stream courant\n",
        "- Faite la copie GPU vers CPU du résultat **d_y** dans **h_y** via **cudaMemcpyAsync()**.\n",
        "\n",
        "Pour rappel, chaque streams s'occupe d'un sous-ensemble du vecteur **h_x**. \n",
        "\n",
        "Si **h_x** est divisé en 64 sous-vecteurs et nous n'avons que 8 streams alors les treams 0, 1, 2, ..., 7 s'occuperont respectivement des sous-vecteurs 0, 1, 2, ... 7 puis des sous-vecteurs 8, 9, 10, ..., 15 et ainsi de suite... \n",
        "Pour les streams pensez à utiliser un offset, pour retourver l'id du stream vous pouvez utiliser le module % :\n",
        "\n",
        "- 0%4 = 0, 1%4 = 1, 2%4 = 2, 3%4 = 3\n",
        "- 4%4 = 0, 5%4 = 1, 6%4 = 2, 7%4 = 3\n",
        "\n",
        "Chaque sous-vecteur est de taille 1024x1024, donc : \n",
        "\n",
        "- Le stream 0 s'occupera du sous-vecteur 0 de taille 1024x1024 commençant par l'indice 0.\n",
        "- Le stream 1 s'occupera du sous-vecteur 1 de taille 1024x1024 commençant par l'indice (1024x1024).\n",
        "- Le stream 2 s'occupera du sous-vecteur 2 de taille 1024x1024 commençant par l'indice 2*(1024x1024).\n",
        "\n",
        "Si vous exécuté le code, une étape de vérification sera effectué pour s'assurer que votre implémentation est correcte.\n",
        "\n",
        "#### 2.6 Execution asynchrone\n",
        "\n",
        "Pour executer le code en mode streams, c'est à dire la partie délimité par **#ifdef USE_STREAM #endif**, rajouter le flag -DUSE_STREAMS dans le makefile.\n",
        "\n",
        "```c\n",
        "NVCC_FLAGS = -DUSE_STREAMS -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_87,code=sm_87\n",
        "```\n",
        "Executez le code et comparez les temps d'execution non-streams vs. streams.\n",
        "\n",
        "Que constatez-vous ?\n",
        "\n",
        "#### 2.7 Modifiez les paramètres\n",
        "```c\n",
        "typedef float ft;\n",
        "const int sub_parts = 64;\n",
        "const size_t ds = 1024*1024*sub_parts;\n",
        "const int count = 22;\n",
        "const int num_streams = 8;\n",
        "```\n",
        "Que se passe t'il au niveau du temps d'execution lorsque vous :\n",
        "\n",
        "1. Changez **typedef float ft** par **typedef double ft** ?\n",
        "2. Augmentez ou diminuez de manière considerable **sub_parts** ?\n",
        "3. Changez la taille **ds** ?\n",
        "3. Augmentez ou diminuez **count** ? \n",
        "4. Augmentez ou diminuez **num_streams** ? (nombre limité par GPU)\n",
        "\n"
      ],
      "metadata": {
        "id": "hrMijrhjbjJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile program.cu\n",
        "#include <math.h>\n",
        "#include <iostream>\n",
        "#include <time.h>\n",
        "#include <sys/time.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "typedef float ft;\n",
        "const int sub_parts = 64;\n",
        "const size_t ds = 1024*1024*sub_parts;\n",
        "const int count = 22;\n",
        "const int num_streams = 8;\n",
        "\n",
        "const float sqrt_2PIf = 2.5066282747946493232942230134974f;\n",
        "const double sqrt_2PI = 2.5066282747946493232942230134974;\n",
        "__device__ float gpdf(float val, float sigma) {\n",
        "  return expf(-0.5f * val * val) / (sigma * sqrt_2PIf);\n",
        "}\n",
        "\n",
        "__device__ double gpdf(double val, double sigma) {\n",
        "  return exp(-0.5 * val * val) / (sigma * sqrt_2PI);\n",
        "}\n",
        "\n",
        "//  calcul la moyenne de la densite de probabilite sur un interval de valeurs autour de chaque point.\n",
        "__global__ void gaussian_pdf(const ft * __restrict__ x, ft * __restrict__ y, const ft mean, const ft sigma, const int n) {\n",
        "  int idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "  if (idx < n) {\n",
        "    ft in = x[idx] - (count / 2) * 0.01f;\n",
        "    ft out = 0;\n",
        "    for (int i = 0; i < count; i++) {\n",
        "      ft temp = (in - mean) / sigma;\n",
        "      out += gpdf(temp, sigma);\n",
        "      in += 0.01f;\n",
        "    }\n",
        "    y[idx] = out / count;\n",
        "  }\n",
        "}\n",
        "\n",
        "// Verification d'erreur CUDA\n",
        "#define cudaCheckErrors(msg) \\\n",
        "  do { \\\n",
        "    cudaError_t __err = cudaGetLastError(); \\\n",
        "    if (__err != cudaSuccess) { \\\n",
        "        fprintf(stderr, \"Fatal error: %s (%s at %s:%d)\\n\", \\\n",
        "            msg, cudaGetErrorString(__err), \\\n",
        "            __FILE__, __LINE__); \\\n",
        "        fprintf(stderr, \"*** FAILED - ABORTING\\n\"); \\\n",
        "        exit(1); \\\n",
        "    } \\\n",
        "  } while (0)\n",
        "\n",
        "// Calcul du temps sur l'host\n",
        "#define USECPSEC 1000000ULL\n",
        "\n",
        "unsigned long long dtime_usec(unsigned long long start) {\n",
        "  timeval tv;\n",
        "  gettimeofday(&tv, 0);\n",
        "  return ((tv.tv_sec*USECPSEC)+tv.tv_usec)-start;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  ft *h_x, *d_x, *h_y, *h_y1, *d_y;\n",
        "  h_x = (ft *)malloc(ds*sizeof(ft));\n",
        "  h_y = (ft *)malloc(ds*sizeof(ft));\n",
        "  h_y1 = (ft *)malloc(ds*sizeof(ft));\n",
        "\n",
        "  cudaMalloc(&d_x, ds*sizeof(ft));\n",
        "  cudaMalloc(&d_y, ds*sizeof(ft));\n",
        "  cudaCheckErrors(\"allocation error\");\n",
        "\n",
        "  gaussian_pdf<<<(ds + 255) / 256, 256>>>(d_x, d_y, 0.0, 1.0, ds); // warm-up\n",
        "\n",
        "  for (size_t i = 0; i < ds; i++) {\n",
        "    h_x[i] = rand() / (ft)RAND_MAX;\n",
        "  }\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  unsigned long long et1 = dtime_usec(0);\n",
        "\n",
        "  cudaMemcpy(d_x, h_x, ds * sizeof(ft), cudaMemcpyHostToDevice);\n",
        "  gaussian_pdf<<<(ds + 255) / 256, 256>>>(d_x, d_y, 0.0, 1.0, ds);\n",
        "  cudaMemcpy(h_y1, d_y, ds * sizeof(ft), cudaMemcpyDeviceToHost);\n",
        "  cudaCheckErrors(\"non-streams execution error\");\n",
        "\n",
        "  et1 = dtime_usec(et1);\n",
        "  std::cout << \"non-stream elapsed time: \" << et1/(float)USECPSEC << std::endl;\n",
        "\n",
        "#ifdef USE_STREAMS\n",
        "  cudaMemset(d_y, 0, ds * sizeof(ft));\n",
        "\n",
        "  unsigned long long et = dtime_usec(0);\n",
        "\n",
        "  // 2.3 Creation des streams\n",
        " \n",
        "\n",
        "  // 2.5 Execution des streams\n",
        "  \n",
        "\n",
        "\n",
        "  et = dtime_usec(et);\n",
        "\n",
        "  for (int i = 0; i < ds; i++) {\n",
        "    if (h_y[i] != h_y1[i]) {\n",
        "      std::cout << \"mismatch at \" << i << \" was: \" << h_y[i] << \" should be: \" << h_y1[i] << std::endl;\n",
        "      return -1;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // 2.4 Destruction des streams\n",
        "\n",
        "  std::cout << \"streams elapsed time: \" << et/(float)USECPSEC << std::endl;\n",
        "#endif\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "QKhzrIKZiMcJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}